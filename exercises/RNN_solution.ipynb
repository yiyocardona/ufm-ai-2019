{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level LSTM en PyTorch\n",
    "\n",
    "En este notebook, vamos a construir un character-level LSTM con PyTorch. La red va se va a entrenar caracter por caracter en algun texto, luego va a generar nuevo texto por caracteres. Como ejemplo, vamos a entrenarlo en Anna Karenina. **Este modelo va a ser capaz de generar texto nuevo basado en texto del libro**\n",
    "\n",
    "Esta red esta basada en el post de Andrej Karpathy [sobre RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) y [su implementacion en Torch](https://github.com/karpathy/char-rnn). Debajo esta la arquitectura general de un character-wise RNN.\n",
    "\n",
    "<img src=\"../assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar la data\n",
    "\n",
    "Primero, vamos a cargar el archivo de texto de Anna Karenina y convertirlo a enteros para ser usado por nuestra red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('../data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miremos los primeros 100 caracteres, para asegurarnos que leimos bien la data. De acuerdo a , [American Book Review](http://americanbookreview.org/100bestlines.asp) esta es la 6ta mejor primera linea de un libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "En las celdas de abajo, vamos a crear un par de **dictionaries** para convertir los caracteres a enteros y de regreso a caracteres. Codificar los caracteres como enteros hace mas facil usarlos como input a una red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los mismos caracteres de arriba, codificados como enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 21, 36, 71, 10,  2, 24, 30, 61, 11, 11, 11, 38, 36, 71, 71, 28,\n",
       "       30,  7, 36, 37, 25, 66, 25,  2, 59, 30, 36, 24,  2, 30, 36, 66, 66,\n",
       "       30, 36, 66, 25, 73,  2, 45, 30,  2, 65,  2, 24, 28, 30,  4, 22, 21,\n",
       "       36, 71, 71, 28, 30,  7, 36, 37, 25, 66, 28, 30, 25, 59, 30,  4, 22,\n",
       "       21, 36, 71, 71, 28, 30, 25, 22, 30, 25, 10, 59, 30, 57, 27, 22, 11,\n",
       "       27, 36, 28, 13, 11, 11, 34, 65,  2, 24, 28, 10, 21, 25, 22])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesamiento de la data\n",
    "\n",
    "Como pueden ver en la imagen del char-RNN arriba, nuestro modelo espera un input que es **one-hot encoded** lo que quiere decir que cada caracter es convertido a un entero (a traves del diccionario que creamos) y _luego_ convertido a un vector columna donde solo su indice entero correspondiente va a tener el valor de 1 y el resto del vector va a estar lleno de 0's. Hagamos una funcion para hacer el one-hot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'casa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7b7d0aea7b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcasa\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mla\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'casa' is not defined"
     ]
    }
   ],
   "source": [
    "casa [1,2,3,4]\n",
    "la [1,2,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacer los mini-batches para entrenamiento\n",
    "\n",
    "\n",
    "Para entrenar en esta data, tambien necesitamos crear mini-batches para entrenamiento. Recuerden que queremos que nuestros batches sean multiples secuencias de un numero deseado de pasos. Considerando un ejemplo simple, nuestros batches se verian de esta forma:\n",
    "\n",
    "<img src=\"../assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "En este ejemplo, vamos a tomar los caracteres codificados (ingresados como el parametro `arr`) y partirlos en multpiles secuencias, dadas por el `batch_size`. Cada una de nuestras secuencias va a ser de largo `seq_length`.\n",
    "\n",
    "### Creando Batches\n",
    "\n",
    "**1. Lo primero que tenemos que hacer es descartar partes del texto para tener unicamente mini-batches completamente llenos.**\n",
    "\n",
    "Cada batch contiene $N \\times M$ caracteres, donde $N$ es el batch size (el numero de secuencias en un batch) y $M$ es el `seq_length` o numero de time steps en una sequencia. Luego, para obtener el numero total de batches, $K$ que podemos hacer del array `arr`, dividimos el length de `arr` por el numero de caracteres por batch. Una vez sabemos el numero de batches, podemos obtener el numero total de caracteres que mantendremos de `arr`, $N * M * K$.\n",
    "\n",
    "**2. Luego, necesitamos partir `arr` en $N$ batches.**\n",
    "\n",
    "Podemos hacer esto usando `arr.reshape(size)` donde `size` es un tuple que contiene las dimensiones del array luego del reshape. Sabemos que queremos $N$ secuencias en un batch, asi que hagamos eso el tamanio de nuestra primera dimension. Para la segunda dimension, podemos usar `-1` como un placeholder en el `size`, esto va a llenar el array con la data apropiada por si solo. Despues, deberiamos tener un array que sea $N \\times (M * K)$.\n",
    "\n",
    "**3. Ahora que tenemos este array, podemos iterar a traves de el para obtener nuestros mini-batches.**\n",
    "\n",
    "La idea es que cada batch es una ventana $N \\times M$ en el array $N \\times (M * K)$. Para cada batch subsecuente, la ventana se mueve por `seq_length`. Tambien queremos crear los arrays para el input y el target. Recuerden que los targets son solo los inputs corridos por un caracter. Mi preferencia personal para hacer esta ventana es usar `range` para tomar pasos de tamanio `n_steps` de $0$ a `arr.shape[1]`, el numero total de tokens en cada secuencia. De esa forma, los enteros que obtenemos de `range` siempre apuntan al principio de un batch, y cada ventana es `seq_length` wide.\n",
    "\n",
    "\n",
    "> **TODO:** Escriban el codigo para crear batches en la funcion de abajo. Los ejercicios en estos notebooks _no van a ser faciles_. No se preocupen si se traban o se tardan, sigan intentando. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1985223"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4963"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded)//(8*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueben su implementacion\n",
    "\n",
    "Vamos a crear unos data sets y podemos revisar que esta pasando mientras hacemos los batches de data. Aqui, como ejemplo, vamos a usar un batch size de 8 y 50 pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[23 21 36 71 10  2 24 30 61 11]\n",
      " [59 57 22 30 10 21 36 10 30 36]\n",
      " [ 2 22 41 30 57 24 30 36 30  7]\n",
      " [59 30 10 21  2 30 67 21 25  2]\n",
      " [30 59 36 27 30 21  2 24 30 10]\n",
      " [67  4 59 59 25 57 22 30 36 22]\n",
      " [30 46 22 22 36 30 21 36 41 30]\n",
      " [48 26 66 57 22 59 73 28 13 30]]\n",
      "\n",
      "y\n",
      " [[21 36 71 10  2 24 30 61 11 11]\n",
      " [57 22 30 10 21 36 10 30 36 10]\n",
      " [22 41 30 57 24 30 36 30  7 57]\n",
      " [30 10 21  2 30 67 21 25  2  7]\n",
      " [59 36 27 30 21  2 24 30 10  2]\n",
      " [ 4 59 59 25 57 22 30 36 22 41]\n",
      " [46 22 22 36 30 21 36 41 30 59]\n",
      " [26 66 57 22 59 73 28 13 30 56]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si implementaron `get_batches` correctamente, el output de arriba deberia verse algo como\n",
    "```\n",
    "x\n",
    " [[25  8 60 11 45 27 28 73  1  2]\n",
    " [17  7 20 73 45  8 60 45 73 60]\n",
    " [27 20 80 73  7 28 73 60 73 65]\n",
    " [17 73 45  8 27 73 66  8 46 27]\n",
    " [73 17 60 12 73  8 27 28 73 45]\n",
    " [66 64 17 17 46  7 20 73 60 20]\n",
    " [73 76 20 20 60 73  8 60 80 73]\n",
    " [47 35 43  7 20 17 24 50 37 73]]\n",
    "\n",
    "y\n",
    " [[ 8 60 11 45 27 28 73  1  2  2]\n",
    " [ 7 20 73 45  8 60 45 73 60 45]\n",
    " [20 80 73  7 28 73 60 73 65  7]\n",
    " [73 45  8 27 73 66  8 46 27 65]\n",
    " [17 60 12 73  8 27 28 73 45 27]\n",
    " [64 17 17 46  7 20 73 60 20 80]\n",
    " [76 20 20 60 73  8 60 80 73 17]\n",
    " [35 43  7 20 17 24 50 37 73 36]]\n",
    " ```\n",
    " aunque los numeros exactos sean diferentes. Asegurense que la data este corrida un paso para `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Definiendo la red con PyTorch\n",
    "\n",
    "Abajo es donde deben definir la red.\n",
    "\n",
    "<img src=\"../assets/charRNN.png\" width=500px>\n",
    "\n",
    "Luego, van a usar PyTorch para definir la arquitectura de la red. Empezamos definiendo los layers y operaciones que queremos. Despues, definimos el metodo para el forward pass. Tambien tienen un metodo para predecir caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura del modelo\n",
    "\n",
    "En `__init__` la estructura sugerida es la siguiente:\n",
    "* Crear y almacenar los diccionarios necesarios (esto ya esta hecho)\n",
    "* Definir un layer LSTM que tomo como params: un input size (el numero de caracteres), un hidden layer size `n_hidden`, un numero de layers `n_layers`, una probabilidad de dropout `drop_prob`, y un booleano `batch_first` (True, ya que estamos batching)\n",
    "* Definir un dropout layer con `drop_prob`\n",
    "* Definir un fully-connected layer con params: input size `n_hidden` y output size (el numero de caracteres)\n",
    "* Finalmente, inicializar los pesos (de nuevo, esto ya esta hecho)\n",
    "\n",
    "Noten que algunos parametros han sido nombrados y dados en la funcion `__init__`, y los usamos y almacenamos haciendo algo como `self.drop_prob = drop_prob`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### LSTM Inputs/Outputs\n",
    "\n",
    "Pueden crar un [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) de la siguiente forma:\n",
    "\n",
    "```python\n",
    "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "```\n",
    "donde `input_size` es el numero de caracteres que este cell espera ver como input secuencial y `n_hidden` es el numero de unidades en los hidden layers en la cell. Podemos agregar dropout agregando un parametro dropout con una probabilidad especificada; esto automaticamente va a agregar dropout a los inputs o outputs. Finalmente, en la funcion `forward`, podemos apilar las LSTM cells en layers usando `.view`. Con esto, pasamos una lista de cells y va a enviar el ouput de una cell a la siguiente cell.\n",
    "\n",
    "Tambien tenemos que crear un hidden state inicial de todos ceros. Esto se hace de la siguiente forma:\n",
    "\n",
    "```python\n",
    "self.init_hidden()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hora de entrenar\n",
    "\n",
    "La funcion de train nos da la habilidad de establecer el numero de epochs, el learning rate y otros parametros.\n",
    "\n",
    "Abajo estamos usando el optimizador Adam y cross entropy loss ya que estamos viendo los scores de las clases de caracteres como output. Podemos calcular el loss y realizar backpropagation como siempre.\n",
    "\n",
    "Un par de detalles sobre el training:\n",
    ">* Dentro del loop de batch, hacemos detach del hidden state de su historia; esta vez lo hacemos asignandolo a una nueva variable (un tuple) porque un LSTM tiene un hidden state que es un tuple de hidden y cell states.\n",
    "\n",
    "* Usamos [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) para prevenir exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializando el modelo\n",
    "\n",
    "Ahora podemos entrenar la red. Primero tenemos que crear la red en si, con sus hiper parametros. Luego, definir el tamanio de los mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializar los hiper parametros de entrenamiento!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo el mejor modelo\n",
    "\n",
    "Para elegir los hiper parametros para obtener el mejor desempenio, hay que ponerle atencion al training y validation loss. Si el training loss es mucho mas bajo que el validation loss, estamos overfitting. En ese caso hay que incrementar la regularizacion (mas dropout) o usar una red mas pequenia. Si el training y validation loss estan cerca, estamos underfitting. En este caso, hay que incrementar el tamanio de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparametros\n",
    "\n",
    "Aqui estan los hiperparametros para la red.\n",
    "\n",
    "Al definir el modelo:\n",
    "* `n_hidden` - Numero de unidades en los hidden layers.\n",
    "* `n_layers` - Numero de hidden LSTM layers para usar.\n",
    "\n",
    "En este ejemplo asumimos que la probabilidad de dropout y el learning rate se van a mantener en sus defaults.\n",
    "\n",
    "en entrenamiento:\n",
    "* `batch_size` - Numero de secuencias corriendo a traves de la red en un forward pass.\n",
    "* `seq_length` - Numero de caracteres en la secuencia en la que la red esta siendo entrenada. Normalmente mas larga es mejor, la red va a aprender dependencias de rangos mas largos. Pero toma mas tiempo para entrenar. 100 es un buen numero aqui. \n",
    "* `lr` - Learning rate para entrenamiento.\n",
    "\n",
    "Aqui pueden leer buenos consejos de Andrej Karpathy para entrenar la red. Lo voy a copy paste para su beneficio, pero tambien proveo el link [de donde originalmente vino](https://github.com/karpathy/char-rnn#tips-and-tricks). A continuacion el copy paste:\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `n_hidden` and `n_layers`. I would advise that you always use `n_layers` of either 2/3. The `n_hidden` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `n_hidden` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "\n",
    "Luego de entrenar, vamos a guardar el modelo para poder cargarlo mas tarde si lo necesitamos. Estamos guardando los parametros necesitados para crear la misma arquitectura, los hiperparametros del hidden layer y los caracteres del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-af7cd6da1328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rnn_x_epoch.net'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m checkpoint = {'n_hidden': net.n_hidden,\n\u001b[0m\u001b[1;32m      5\u001b[0m               \u001b[0;34m'n_layers'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m               \u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_x_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Haciendo predicciones\n",
    "\n",
    "Ahora que el modelo esta entrenado, vamos a querer muestrear de el y hacer predicciones sobre caracteres. Para muestrear, pasamos un caracter y hacemos que el modelo prediga el siguiente caracter. Luego tomamos ese caracter, lo metemos de nuevo, y obtenemos otro caracter predecido. Hacemos esto varias veces para generar mas texto.\n",
    "\n",
    "### Una nota sobre la funcion `predict`\n",
    "\n",
    "El output de nuestra RNN es de una fully-connected layer y saca una **distribucion de scores para el siguiente caracter**\n",
    "\n",
    "> Para sacar el siguiente caracter, aplicamos una funcion softmax, la cual nos da una distribucion de _probabilidades_ del cual podemos muestrear para predecir el siguiente caracter.\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "Nuestras predicciones vienen de una distribucion de probabilidades categoricas sobre todos los posibles caracteres. Podemos hacer el texto de la muestra y hacerlo mas facil de manejar (con menos variables) considerando unicamente algunos $K$ caracteres mas probables. Esto va a prevenir que la red nos de caracteres completamente absurdos y a la vez permitir que la red introduzca un poco de ruido y randomness al texto muestrado. Pueden leer mas sobre [topk, aqui](https://pytorch.org/docs/stable/torch.html#torch.topk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priming y generando texto\n",
    "\n",
    "Normalmente vamos a querer prime a la red para poder construir su hidden state. De lo contrario la red va a empezar generando caracteres de forma aleatoria. En general el primer monton de caracteres va a ser mas malo ya que no ha construido una historia suficientemente larga de caracteres de los cuales puede formar predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando un checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_x_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
